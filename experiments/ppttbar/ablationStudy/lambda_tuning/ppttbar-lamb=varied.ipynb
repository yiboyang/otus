{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTUS |  $ p p > t \\bar{t} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks applies OTUS to our second test case: semi-leptonic $t \\bar{t}$ decay.\n",
    "\n",
    "Our physical latent-space is the $e^-$, $\\bar{\\nu}_e$, $b$, $\\bar{b}$, $u$, $\\bar{d}$ 4-momentum information produced by the program MadGraph.\n",
    "\n",
    "Our data-space data is the $e^+$, $MET$, $jet1$, $jet2$, $jet3$, $jet4$ 4-momentum information produced by the program Delphes. The jets are ordered in descending $p_T$.\n",
    "\n",
    "We arrange this information into 24 dimensional vectors\n",
    "\n",
    "- Latent space (z): [$p^{\\mu}_{e-}$,$p^{\\mu}_{\\bar{\\nu}_e}$,$p^{\\mu}_{b}$,$p^{\\mu}_{\\bar{b}}$,$p^{\\mu}_{u}$,$p^{\\mu}_{\\bar{d}}$]\n",
    "- Data space (x): [$p^{\\mu}_{e^-}$,$p^{\\mu}_{MET}$,$p^{\\mu}_{jet1}$,$p^{\\mu}_{jet2}$,$p^{\\mu}_{jet3}$,$p^{\\mu}_{jet4}$]\n",
    "\n",
    "where $p^{\\mu}=[p_x, p_y, p_z, E]$ is the 4-momentum of the given particle.\n",
    "\n",
    "###### Additional Losses and Constraints:\n",
    "We impose the following additional losses and constraints in this problem.\n",
    "\n",
    "As in the $p p > Z > e^+ e^-$ test case, we explicitly enforce the Minkowski metric in the output of the networks. Namely, the networks predict the 3-momenta ($\\vec{p}$) of the particles. Energy information is then restored using the Minkowski metric: $E^2 = |\\vec{p}|^2 + m^2$.\n",
    "\n",
    "We also explicitly enforce the lower $p_T$ threshold on jets, which requires that $p_T>20$ GeV. Only samples generated by the decoder which pass this threshold are used to calculate losses. This requires modifying the data-space loss term slightly. Additionally, to help with stable traiing, we choose a ResNet architecture for both our encoder and decoder networks.\n",
    "\n",
    "See the paper for more details: https://arxiv.org/abs/2101.08944."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "root_dir = '../../../'\n",
    "\n",
    "#-- Add utilityFunctions/ to easily use utility .py files --#\n",
    "import sys\n",
    "sys.path.append(os.path.join(root_dir, \"utilityFunctions/\"))\n",
    "\n",
    "#-- Determine if using GPU or CPU --#\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # Set to '-1' to disable GPU\n",
    "from configs import device, data_dims\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data type:  float32\n"
     ]
    }
   ],
   "source": [
    "data_directory    = os.path.join(root_dir, \"data/\")\n",
    "dataset_name      = 'ppttbar'\n",
    "\n",
    "#-- Set random seeds --#\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#-- Raw or standardized inputs/outputs --#\n",
    "# If True, model inputs/outputs should be in the \"raw\" (unstandardized) space\n",
    "raw_io = True  \n",
    "\n",
    "#-- Set data type --#\n",
    "from configs import float_type\n",
    "print('Using data type: ', float_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data total shapes:  (262761, 24) (262761, 24)\n",
      "z_train shape, x_train shape:  (222761, 24) (222761, 24)\n",
      "z_val   shape, x_val   shape:  (40000, 24) (40000, 24)\n"
     ]
    }
   ],
   "source": [
    "from func_utils import get_dataset, standardize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#-- Get training and validation dataset --#\n",
    "dataset = get_dataset(dataset_name, data_dir=data_directory)\n",
    "z_data, x_data = dataset['z_data'], dataset['x_data']\n",
    "print(\"Data total shapes: \",z_data.shape, x_data.shape)\n",
    "\n",
    "x_dim = int(x_data.shape[1])\n",
    "z_dim = int(z_data.shape[1])\n",
    "\n",
    "#-- Split into training and validation sets --#\n",
    "train_size = 222761\n",
    "val_size = 40000  # Validation set used to evaluate/tune models\n",
    "\n",
    "x_train = x_data[:train_size, :]\n",
    "x_val = x_data[train_size:train_size+val_size, :]\n",
    "\n",
    "z_train = z_data[:train_size, :]\n",
    "z_val = z_data[train_size:train_size+val_size, :]\n",
    "\n",
    "#-- Convert data to proper type --#\n",
    "x_train, x_val, z_train, z_val = list(map(lambda x: x.astype(float_type), [x_train, x_val, z_train, z_val]))\n",
    "\n",
    "#-- Obtain mean and std information --#\n",
    "# This is needed to standardize/unstandardize data\n",
    "x_train_mean, x_train_std = np.mean(x_train, axis=0), np.std(x_train, axis=0)\n",
    "z_train_mean, z_train_std = np.mean(z_train, axis=0), np.std(z_train, axis=0)\n",
    "\n",
    "# If raw_io == False, then standardize the data with training set statistics\n",
    "if not raw_io:\n",
    "    x_train = (x_train - x_train_mean) / x_train_std  \n",
    "    x_val = (x_val - x_train_mean) / x_train_std  \n",
    "if not raw_io:\n",
    "    z_train = (z_train - z_train_mean) / z_train_std\n",
    "    z_val = (z_val - z_train_mean) / z_train_std\n",
    "\n",
    "#-- Set evaluation parameters --#\n",
    "eval_batch_size = 20000  # Always use high batch size on validation set to accurately assess performance\n",
    "eval_loaders = DataLoader(dataset=x_val, batch_size=eval_batch_size, shuffle=True), \\\n",
    "               DataLoader(dataset=z_val, batch_size=eval_batch_size, shuffle=True)\n",
    "\n",
    "print(\"z_train shape, x_train shape: \", z_train.shape, x_train.shape)\n",
    "print(\"z_val   shape, x_val   shape: \", z_val.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target invariant masses (for both training and validation data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invariant mass relation: $m^2 = E^2 - |\\vec{p}|^2$. For objects with ill-defined mass (MET and Jets) we fix $m=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inv_masses = np.zeros(6)\n",
    "z_inv_masses = np.array([0., 0., 4.7, 4.7, 0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Specific Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from ppttbar_constraints import threshold_check\n",
    "from ppttbar_utils import train_and_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Meta Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoencoder, StochasticResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Latent loss function:\n",
    "Finite sample approximation of Sliced Wasserstein Distance (SWD) between $p(z)$ and $p_E(z) = \\int_x p(x) p_E(z|x)$\n",
    "\n",
    "- $L_{latent}(Z, \\tilde{Z}) = \\frac{1}{L * M} \\sum_{l=1}^{L} \\sum_{m=1}^{M} c((\\theta_l \\cdot z_m)_{sorted}, (\\theta_l \\cdot \\tilde{z}_m)_{sorted})$\n",
    "\n",
    "where $c(\\cdot, \\cdot) = |\\cdot - \\cdot|^2$\n",
    "\n",
    "###### Data loss function:\n",
    "- $L_{data}(X, \\tilde{X}) = \\frac{1}{M} \\sum_{m=1}^M [\\frac{1_S(\\tilde{x}_m)}{p_D(S)} \n",
    "c(x_m,  \\tilde{x}_m)]$\n",
    "\n",
    "where $c(\\cdot, \\cdot) = |\\cdot - \\cdot|^2$; $1_S(x)$ is the indicator function of $S$ so that it equals $1$ if $x \\in S$, and $0$ otherwise, and $p_D(S) := \\int dt p_D(t) 1_S(t)$ normalizes this distribution.\n",
    "\n",
    "###### Full loss function:\n",
    "- $L_{tot} = \\beta L_{data}(X, \\tilde{X}) + \\lambda L_{latent}(Z, \\tilde{Z})$ \n",
    "\n",
    "###### Core Hyperparameters\n",
    "The hyperparameter definitions are as follows:\n",
    "\n",
    "- num_hidden_layers: The number of hidden layers in both the encoder and decoder networks\n",
    "- dim_per_hidden_layer: The dimensions per hidden layer in both the encoder and decoder networks\n",
    "- lr: The learning rate of the networks\n",
    "- lamb: The $\\lambda$ coefficient in front of the latent loss term\n",
    "- num_slices: Number of random projections used for computing SWD\n",
    "- epochs: The number of epochs used during training\n",
    "\n",
    "Hyperparameters for other losses that were tried, but use during main training is currently discouraged:\n",
    "\n",
    "- tau: Coefficient in front of the alternate data-space loss (\"alt_x_loss\"), which is the SWD between $p(x)$ and $p_D(x):=\\int_z p(z) p_D(x|z)$\n",
    "- rho: Coefficient in front of an additional decoder constraint loss (based on soft-penalty approach to learning hard thresholds/ttbar_constraints)\n",
    "\n",
    "###### Joint Training Hyperparameters\n",
    "- beta: Coefficient in front of data loss, $L_{data}$ \n",
    "- beta_e: Coefficient in front of the encoder \"anchor loss\" \n",
    "- beta_d: Coefficient in front of the decoder \"anchor loss\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: most of the unspecified hyperparameters are set to 0 by default\n",
    "\n",
    "# common configs\n",
    "joint_step_config = {\n",
    "    'lr': 0.001,\n",
    "    'beta': 1.,  # coefficient in front of data loss, E[c(x, x reconstructed)], where c is typically the 2-norm.\n",
    "    'lamb': 20.,  # coefficient in front of latent loss, SWD between p(z) and Q(z):=\\int_x p(x) Q(z|x)\n",
    "    'tau': 0,  # coefficient in front of \"alt_x_loss\", which is the SWD between p(x) and p_G(x):=\\int_z p(z) p_G(x|z);\n",
    "               # this loss is not part of the original WAE formulation and is not used.\n",
    "    'rho': 0, # coef in front of decoder constraint loss (based on soft-penalty approach to learning hard thresholds/ttbar_constraints)\n",
    "    'nu_e': 0,  # coefficient in front of encoder \"anchor loss\"\n",
    "    'nu_d': 0,\n",
    "    'epochs': 1000,\n",
    "    'log_freq': 100,\n",
    "}\n",
    "\n",
    "\n",
    "decoder_finetuning_config = {\n",
    "    'beta': 0,  # coefficient in front of data loss in (S)WAE objective\n",
    "    'tau': 1, # coefficient in front of \"alt_x_loss\", which is the SWD between p(x) and p_D(x)\n",
    "    'lamb': 0, # disable latent loss\n",
    "    'rho': 0, # no x_constraint_loss (no longer used)\n",
    "    'nu_e': 0,  # anchor loss\n",
    "    'nu_d': 0,\n",
    "    'lr': 0.0001,  # reduced lr for fine-tuning\n",
    "    'epochs': 10,\n",
    "    'log_freq': 1,\n",
    "}\n",
    "\n",
    "\n",
    "hidden_layer_dims = [64, 64]\n",
    "activation = torch.nn.ReLU\n",
    "from models import Autoencoder, StochasticResNet\n",
    "model = Autoencoder(x_dim, z_dim, ConditionalModel=StochasticResNet, encoder_hidden_layer_dims=hidden_layer_dims,\n",
    "                    stoch_enc=True, stoch_dec=True, activation=activation, raw_io=raw_io,\n",
    "                    x_inv_masses=x_inv_masses, x_stats=np.stack([x_train_mean, x_train_std]),\n",
    "                    z_inv_masses=z_inv_masses, z_stats=np.stack([z_train_mean, z_train_std]),\n",
    "                    # ResNet settings:\n",
    "                    io_residual=True,\n",
    "                    res_mlp_depth=2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): StochasticResNet(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ResBlock(\n",
       "        (module): Sequential(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=64, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): StochasticResNet(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ResBlock(\n",
       "        (module): Sequential(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=64, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 20000\n",
    "train_loaders = DataLoader(dataset=x_train, batch_size=train_batch_size, shuffle=True), \\\n",
    "                DataLoader(dataset=z_train, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# Note that the Z and X data loaders are both shuffled, so (z, x) samples no longer match up (unlike in the source\n",
    "# data arrays), as is demanded by our unsupervised problem setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through different hyperparameters and train/eval a model for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lamb=0.001\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 0.001, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:5503.24, prate:0.681, x_loss:5465.75, z_loss:37496.6, anchor_loss:0\n",
      "eval -- loss:78743.2, prate:0.671, x_loss:4721.69, z_loss:40052.5, alt_x_loss:38690.7, anchor_loss:0.442782\n",
      "epoch:\t100\n",
      "train -- loss:32.2845, prate:0.681, x_loss:3.19206, z_loss:29092.5, anchor_loss:0\n",
      "eval -- loss:69477.3, prate:0.672, x_loss:7.37862, z_loss:28035.2, alt_x_loss:41442.1, anchor_loss:0.0736043\n",
      "epoch:\t200\n",
      "train -- loss:10.2287, prate:0.664, x_loss:0.808117, z_loss:9420.58, anchor_loss:0\n",
      "eval -- loss:34573.7, prate:0.681, x_loss:1.2351, z_loss:8471.29, alt_x_loss:26102.4, anchor_loss:0.0695682\n",
      "epoch:\t300\n",
      "train -- loss:5.06961, prate:0.684, x_loss:1.46862, z_loss:3600.99, anchor_loss:0\n",
      "eval -- loss:20625.1, prate:0.692, x_loss:1.49353, z_loss:3461.33, alt_x_loss:17163.8, anchor_loss:0.173854\n",
      "epoch:\t400\n",
      "train -- loss:2.77892, prate:0.69, x_loss:0.49184, z_loss:2287.08, anchor_loss:0\n",
      "eval -- loss:16432.2, prate:0.7, x_loss:1.01282, z_loss:2427.42, alt_x_loss:14004.8, anchor_loss:0.221434\n",
      "epoch:\t500\n",
      "train -- loss:2.28761, prate:0.684, x_loss:0.396815, z_loss:1890.79, anchor_loss:0\n",
      "eval -- loss:13892.1, prate:0.704, x_loss:0.61323, z_loss:1822.14, alt_x_loss:12069.9, anchor_loss:0.240789\n",
      "epoch:\t600\n",
      "train -- loss:2.16156, prate:0.696, x_loss:0.342057, z_loss:1819.5, anchor_loss:0\n",
      "eval -- loss:11921.4, prate:0.707, x_loss:0.357493, z_loss:1418.3, alt_x_loss:10503.1, anchor_loss:0.245217\n",
      "epoch:\t700\n",
      "train -- loss:1.12129, prate:0.709, x_loss:0.0600357, z_loss:1061.25, anchor_loss:0\n",
      "eval -- loss:10636.5, prate:0.709, x_loss:0.0757207, z_loss:1138.22, alt_x_loss:9498.24, anchor_loss:0.250115\n",
      "epoch:\t800\n",
      "train -- loss:1.18242, prate:0.705, x_loss:0.0816627, z_loss:1100.76, anchor_loss:0\n",
      "eval -- loss:10237.9, prate:0.709, x_loss:0.0708314, z_loss:888.255, alt_x_loss:9349.62, anchor_loss:0.253396\n",
      "epoch:\t900\n",
      "train -- loss:0.682827, prate:0.724, x_loss:0.0303994, z_loss:652.428, anchor_loss:0\n",
      "eval -- loss:9425.25, prate:0.707, x_loss:0.0287182, z_loss:709.021, alt_x_loss:8716.23, anchor_loss:0.258019\n",
      "epoch:\t999\n",
      "train -- loss:1.0213, prate:0.703, x_loss:0.319654, z_loss:701.645, anchor_loss:0\n",
      "eval -- loss:8860.16, prate:0.706, x_loss:0.617309, z_loss:545.388, alt_x_loss:8314.77, anchor_loss:0.265069\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:2056.2, prate:0.714, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:2159.15, prate:0.714, x_loss:7483.41, z_loss:572.254, alt_x_loss:1586.89, anchor_loss:0.247243\n",
      "epoch:\t1\n",
      "train -- loss:1260.54, prate:0.725, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:1500.59, prate:0.716, x_loss:22648.6, z_loss:560.448, alt_x_loss:940.145, anchor_loss:0.221887\n",
      "epoch:\t2\n",
      "train -- loss:1447.4, prate:0.697, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:1050.93, prate:0.705, x_loss:15274.5, z_loss:557.284, alt_x_loss:493.644, anchor_loss:0.205556\n",
      "epoch:\t3\n",
      "train -- loss:796.029, prate:0.698, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:939.931, prate:0.701, x_loss:18538.6, z_loss:556.92, alt_x_loss:383.011, anchor_loss:0.199058\n",
      "epoch:\t4\n",
      "train -- loss:1269.06, prate:0.711, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:873.393, prate:0.698, x_loss:21534.6, z_loss:553.922, alt_x_loss:319.47, anchor_loss:0.191922\n",
      "epoch:\t5\n",
      "train -- loss:651.622, prate:0.672, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:843.002, prate:0.691, x_loss:20027.8, z_loss:541.145, alt_x_loss:301.857, anchor_loss:0.18873\n",
      "epoch:\t6\n",
      "train -- loss:1037.56, prate:0.683, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:844.261, prate:0.689, x_loss:20873.1, z_loss:551.284, alt_x_loss:292.977, anchor_loss:0.18872\n",
      "epoch:\t7\n",
      "train -- loss:812.426, prate:0.675, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:823.109, prate:0.686, x_loss:21912, z_loss:570.969, alt_x_loss:252.14, anchor_loss:0.187389\n",
      "epoch:\t8\n",
      "train -- loss:1017.47, prate:0.673, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:822.318, prate:0.683, x_loss:21993.4, z_loss:542.223, alt_x_loss:280.095, anchor_loss:0.188505\n",
      "epoch:\t9\n",
      "train -- loss:766.328, prate:0.675, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:784.17, prate:0.682, x_loss:22478.2, z_loss:540.662, alt_x_loss:243.508, anchor_loss:0.189327\n",
      "Final val losses: {'passing_rate': 0.6821500062942505, 'x_loss': 22478.2265625, 'z_loss': 540.6615600585938, 'alt_x_loss': 243.50819396972656, 'encoder_anchor_loss': 0.08474725484848022, 'decoder_anchor_loss': 0.10457984358072281, 'loss': 784.1697540283203}\n",
      "Saved model weights to ./swae-lamb=0.001.pkl\n",
      "Passing rate of z_decoded 0.6808\n",
      "Passing rate of x_reconstructed 0.84035\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.6808\n",
      "Passing rate of x_reconstructed 0.84035\n",
      "\n",
      "\n",
      "Training with lamb=0.01\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 0.01, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:5839.92, prate:0.68, x_loss:5467.33, z_loss:37259.1, anchor_loss:0\n",
      "eval -- loss:78542, prate:0.671, x_loss:4722.5, z_loss:39850.4, alt_x_loss:38691.7, anchor_loss:0.442641\n",
      "epoch:\t100\n",
      "train -- loss:17.9206, prate:0.702, x_loss:6.15723, z_loss:1176.33, anchor_loss:0\n",
      "eval -- loss:26688.4, prate:0.692, x_loss:6.87864, z_loss:1030.99, alt_x_loss:25657.4, anchor_loss:0.238017\n",
      "epoch:\t200\n",
      "train -- loss:12.9495, prate:0.666, x_loss:5.21069, z_loss:773.877, anchor_loss:0\n",
      "eval -- loss:25602.3, prate:0.684, x_loss:5.08258, z_loss:490.791, alt_x_loss:25111.5, anchor_loss:0.20522\n",
      "epoch:\t300\n",
      "train -- loss:5.32926, prate:0.674, x_loss:0.895707, z_loss:443.356, anchor_loss:0\n",
      "eval -- loss:27294.7, prate:0.677, x_loss:0.932141, z_loss:368.016, alt_x_loss:26926.7, anchor_loss:0.1865\n",
      "epoch:\t400\n",
      "train -- loss:6.01835, prate:0.678, x_loss:1.16621, z_loss:485.214, anchor_loss:0\n",
      "eval -- loss:26725, prate:0.675, x_loss:0.58846, z_loss:304.477, alt_x_loss:26420.5, anchor_loss:0.169558\n",
      "epoch:\t500\n",
      "train -- loss:7.6687, prate:0.655, x_loss:3.267, z_loss:440.17, anchor_loss:0\n",
      "eval -- loss:25358.7, prate:0.675, x_loss:5.70153, z_loss:232.684, alt_x_loss:25126, anchor_loss:0.16135\n",
      "epoch:\t600\n",
      "train -- loss:4.57275, prate:0.666, x_loss:0.565792, z_loss:400.696, anchor_loss:0\n",
      "eval -- loss:23369.3, prate:0.674, x_loss:0.61598, z_loss:218.472, alt_x_loss:23150.8, anchor_loss:0.153112\n",
      "epoch:\t700\n",
      "train -- loss:3.45048, prate:0.696, x_loss:0.667237, z_loss:278.325, anchor_loss:0\n",
      "eval -- loss:21472.6, prate:0.677, x_loss:0.411157, z_loss:197.335, alt_x_loss:21275.2, anchor_loss:0.154335\n",
      "epoch:\t800\n",
      "train -- loss:4.7384, prate:0.677, x_loss:1.53621, z_loss:320.219, anchor_loss:0\n",
      "eval -- loss:21758.1, prate:0.679, x_loss:2.41181, z_loss:190.079, alt_x_loss:21568.1, anchor_loss:0.150195\n",
      "epoch:\t900\n",
      "train -- loss:2.90709, prate:0.691, x_loss:0.254987, z_loss:265.21, anchor_loss:0\n",
      "eval -- loss:19675.8, prate:0.682, x_loss:0.271686, z_loss:173.321, alt_x_loss:19502.4, anchor_loss:0.152912\n",
      "epoch:\t999\n",
      "train -- loss:3.40644, prate:0.677, x_loss:0.298496, z_loss:310.794, anchor_loss:0\n",
      "eval -- loss:18612.6, prate:0.684, x_loss:0.217438, z_loss:158.227, alt_x_loss:18454.4, anchor_loss:0.151165\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:6719.2, prate:0.701, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:7262.83, prate:0.692, x_loss:7992.88, z_loss:157.984, alt_x_loss:7104.85, anchor_loss:0.150808\n",
      "epoch:\t1\n",
      "train -- loss:1116.74, prate:0.708, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:1561.51, prate:0.7, x_loss:54479.7, z_loss:154.704, alt_x_loss:1406.81, anchor_loss:0.161864\n",
      "epoch:\t2\n",
      "train -- loss:1755.63, prate:0.703, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:636.946, prate:0.693, x_loss:43279.4, z_loss:157.018, alt_x_loss:479.929, anchor_loss:0.144447\n",
      "epoch:\t3\n",
      "train -- loss:931.309, prate:0.693, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:632.461, prate:0.69, x_loss:34644.4, z_loss:164.356, alt_x_loss:468.104, anchor_loss:0.139467\n",
      "epoch:\t4\n",
      "train -- loss:1293.08, prate:0.68, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:478.343, prate:0.685, x_loss:40619.6, z_loss:154.264, alt_x_loss:324.079, anchor_loss:0.1384\n",
      "epoch:\t5\n",
      "train -- loss:669.159, prate:0.673, x_loss:0, z_loss:0, anchor_loss:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval -- loss:505.069, prate:0.684, x_loss:42609.9, z_loss:151.609, alt_x_loss:353.46, anchor_loss:0.141293\n",
      "epoch:\t6\n",
      "train -- loss:1018.81, prate:0.67, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:462.57, prate:0.679, x_loss:41429.5, z_loss:163.64, alt_x_loss:298.93, anchor_loss:0.145193\n",
      "epoch:\t7\n",
      "train -- loss:789.46, prate:0.675, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:417.667, prate:0.677, x_loss:41924.4, z_loss:165.167, alt_x_loss:252.5, anchor_loss:0.148406\n",
      "epoch:\t8\n",
      "train -- loss:645.792, prate:0.676, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:431.832, prate:0.675, x_loss:42712.3, z_loss:156.565, alt_x_loss:275.267, anchor_loss:0.152875\n",
      "epoch:\t9\n",
      "train -- loss:1042.28, prate:0.671, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:402.32, prate:0.67, x_loss:43605.6, z_loss:158.716, alt_x_loss:243.605, anchor_loss:0.155959\n",
      "Final val losses: {'passing_rate': 0.6702749729156494, 'x_loss': 43605.61328125, 'z_loss': 158.71571350097656, 'alt_x_loss': 243.60467529296875, 'encoder_anchor_loss': 0.10629265010356903, 'decoder_anchor_loss': 0.04966630041599274, 'loss': 402.3203887939453}\n",
      "Saved model weights to ./swae-lamb=0.01.pkl\n",
      "Passing rate of z_decoded 0.671225\n",
      "Passing rate of x_reconstructed 0.8223\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.671225\n",
      "Passing rate of x_reconstructed 0.8223\n",
      "\n",
      "\n",
      "Training with lamb=0.1\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 0.1, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:9028.84, prate:0.68, x_loss:5501.6, z_loss:35272.4, anchor_loss:0\n",
      "eval -- loss:76916.2, prate:0.67, x_loss:4694.99, z_loss:38226.7, alt_x_loss:38689.5, anchor_loss:0.443756\n",
      "epoch:\t100\n",
      "train -- loss:46.865, prate:0.681, x_loss:7.97907, z_loss:388.86, anchor_loss:0\n",
      "eval -- loss:25848.1, prate:0.676, x_loss:9.85434, z_loss:238.851, alt_x_loss:25609.3, anchor_loss:0.150014\n",
      "epoch:\t200\n",
      "train -- loss:48.9636, prate:0.658, x_loss:15.3782, z_loss:335.854, anchor_loss:0\n",
      "eval -- loss:18831.3, prate:0.676, x_loss:14.3393, z_loss:147.874, alt_x_loss:18683.5, anchor_loss:0.120764\n",
      "epoch:\t300\n",
      "train -- loss:21.0479, prate:0.673, x_loss:2.52468, z_loss:185.232, anchor_loss:0\n",
      "eval -- loss:15787.2, prate:0.679, x_loss:3.15824, z_loss:115.695, alt_x_loss:15671.5, anchor_loss:0.114193\n",
      "epoch:\t400\n",
      "train -- loss:29.5113, prate:0.671, x_loss:2.4067, z_loss:271.046, anchor_loss:0\n",
      "eval -- loss:12796.6, prate:0.682, x_loss:2.40766, z_loss:96.1508, alt_x_loss:12700.4, anchor_loss:0.106882\n",
      "epoch:\t500\n",
      "train -- loss:29.4422, prate:0.662, x_loss:2.0847, z_loss:273.575, anchor_loss:0\n",
      "eval -- loss:10560.7, prate:0.686, x_loss:2.0217, z_loss:82.3291, alt_x_loss:10478.4, anchor_loss:0.0987375\n",
      "epoch:\t600\n",
      "train -- loss:25.2243, prate:0.689, x_loss:1.04116, z_loss:241.831, anchor_loss:0\n",
      "eval -- loss:8459.17, prate:0.689, x_loss:1.07042, z_loss:76.891, alt_x_loss:8382.28, anchor_loss:0.0923752\n",
      "epoch:\t700\n",
      "train -- loss:21.5515, prate:0.705, x_loss:4.54331, z_loss:170.082, anchor_loss:0\n",
      "eval -- loss:6982.52, prate:0.694, x_loss:5.07111, z_loss:75.7306, alt_x_loss:6906.79, anchor_loss:0.0889749\n",
      "epoch:\t800\n",
      "train -- loss:22.5381, prate:0.695, x_loss:2.3897, z_loss:201.484, anchor_loss:0\n",
      "eval -- loss:6441.35, prate:0.698, x_loss:3.25315, z_loss:73.1931, alt_x_loss:6368.16, anchor_loss:0.0945506\n",
      "epoch:\t900\n",
      "train -- loss:18.8321, prate:0.707, x_loss:4.14834, z_loss:146.838, anchor_loss:0\n",
      "eval -- loss:5359.94, prate:0.702, x_loss:4.86141, z_loss:68.4431, alt_x_loss:5291.49, anchor_loss:0.102459\n",
      "epoch:\t999\n",
      "train -- loss:21.7231, prate:0.711, x_loss:1.15333, z_loss:205.698, anchor_loss:0\n",
      "eval -- loss:4617.88, prate:0.705, x_loss:0.968732, z_loss:64.1934, alt_x_loss:4553.68, anchor_loss:0.112785\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:1504.35, prate:0.711, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:623.913, prate:0.705, x_loss:5390.88, z_loss:62.4031, alt_x_loss:561.509, anchor_loss:0.101417\n",
      "epoch:\t1\n",
      "train -- loss:1102.03, prate:0.703, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:584.96, prate:0.699, x_loss:8556.52, z_loss:62.6677, alt_x_loss:522.292, anchor_loss:0.105076\n",
      "epoch:\t2\n",
      "train -- loss:859.004, prate:0.693, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:465.323, prate:0.69, x_loss:5696.55, z_loss:62.4313, alt_x_loss:402.891, anchor_loss:0.114475\n",
      "epoch:\t3\n",
      "train -- loss:825.16, prate:0.692, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:396.359, prate:0.686, x_loss:6638, z_loss:67.2688, alt_x_loss:329.09, anchor_loss:0.118909\n",
      "epoch:\t4\n",
      "train -- loss:1401.27, prate:0.687, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:352.535, prate:0.683, x_loss:7213.9, z_loss:61.9117, alt_x_loss:290.623, anchor_loss:0.120983\n",
      "epoch:\t5\n",
      "train -- loss:717.676, prate:0.673, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:358.192, prate:0.679, x_loss:7384.44, z_loss:59.8671, alt_x_loss:298.325, anchor_loss:0.122984\n",
      "epoch:\t6\n",
      "train -- loss:1067.7, prate:0.658, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:378.367, prate:0.675, x_loss:7128.4, z_loss:68.7935, alt_x_loss:309.574, anchor_loss:0.125379\n",
      "epoch:\t7\n",
      "train -- loss:853.314, prate:0.676, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:307.998, prate:0.672, x_loss:7455.79, z_loss:65.2686, alt_x_loss:242.73, anchor_loss:0.12639\n",
      "epoch:\t8\n",
      "train -- loss:810.54, prate:0.675, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:335.825, prate:0.667, x_loss:7841.74, z_loss:65.058, alt_x_loss:270.767, anchor_loss:0.128551\n",
      "epoch:\t9\n",
      "train -- loss:938.196, prate:0.665, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:324.834, prate:0.666, x_loss:7763.77, z_loss:64.0287, alt_x_loss:260.805, anchor_loss:0.130575\n",
      "Final val losses: {'passing_rate': 0.6655499935150146, 'x_loss': 7763.77001953125, 'z_loss': 64.02874755859375, 'alt_x_loss': 260.8050231933594, 'encoder_anchor_loss': 0.06767818331718445, 'decoder_anchor_loss': 0.06289699673652649, 'loss': 324.8337707519531}\n",
      "Saved model weights to ./swae-lamb=0.1.pkl\n",
      "Passing rate of z_decoded 0.665075\n",
      "Passing rate of x_reconstructed 0.8214\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.665075\n",
      "Passing rate of x_reconstructed 0.8214\n",
      "\n",
      "\n",
      "Training with lamb=1\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 1, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:34161.5, prate:0.677, x_loss:6145.08, z_loss:28016.4, anchor_loss:0\n",
      "eval -- loss:71166.6, prate:0.67, x_loss:4864.24, z_loss:32865.6, alt_x_loss:38301, anchor_loss:0.460176\n",
      "epoch:\t100\n",
      "train -- loss:228.914, prate:0.692, x_loss:18.0347, z_loss:210.879, anchor_loss:0\n",
      "eval -- loss:9788.02, prate:0.681, x_loss:19.6961, z_loss:88.0732, alt_x_loss:9699.94, anchor_loss:0.0967153\n",
      "epoch:\t200\n",
      "train -- loss:269.436, prate:0.674, x_loss:12.9258, z_loss:256.511, anchor_loss:0\n",
      "eval -- loss:3909.44, prate:0.689, x_loss:10.3402, z_loss:65.943, alt_x_loss:3843.5, anchor_loss:0.0843416\n",
      "epoch:\t300\n",
      "train -- loss:157.492, prate:0.691, x_loss:16.0132, z_loss:141.479, anchor_loss:0\n",
      "eval -- loss:2606.23, prate:0.703, x_loss:20.9946, z_loss:62.4715, alt_x_loss:2543.76, anchor_loss:0.0835104\n",
      "epoch:\t400\n",
      "train -- loss:227.104, prate:0.705, x_loss:9.2752, z_loss:217.828, anchor_loss:0\n",
      "eval -- loss:1856.1, prate:0.717, x_loss:8.40333, z_loss:58.0161, alt_x_loss:1798.08, anchor_loss:0.0900946\n",
      "epoch:\t500\n",
      "train -- loss:228.685, prate:0.724, x_loss:8.20226, z_loss:220.483, anchor_loss:0\n",
      "eval -- loss:1733.18, prate:0.725, x_loss:8.96576, z_loss:51.645, alt_x_loss:1681.54, anchor_loss:0.104046\n",
      "epoch:\t600\n",
      "train -- loss:215.913, prate:0.737, x_loss:18.5653, z_loss:197.347, anchor_loss:0\n",
      "eval -- loss:1430.72, prate:0.738, x_loss:21.5246, z_loss:54.0923, alt_x_loss:1376.63, anchor_loss:0.120281\n",
      "epoch:\t700\n",
      "train -- loss:163.068, prate:0.759, x_loss:6.98669, z_loss:156.081, anchor_loss:0\n",
      "eval -- loss:1484.99, prate:0.754, x_loss:8.06311, z_loss:51.2613, alt_x_loss:1433.72, anchor_loss:0.131719\n",
      "epoch:\t800\n",
      "train -- loss:178.641, prate:0.772, x_loss:9.63864, z_loss:169.002, anchor_loss:0\n",
      "eval -- loss:1523.84, prate:0.773, x_loss:8.03162, z_loss:53.6266, alt_x_loss:1470.22, anchor_loss:0.142129\n",
      "epoch:\t900\n",
      "train -- loss:115.561, prate:0.781, x_loss:6.24489, z_loss:109.316, anchor_loss:0\n",
      "eval -- loss:1471.41, prate:0.786, x_loss:5.40728, z_loss:45.4805, alt_x_loss:1425.93, anchor_loss:0.146544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t999\n",
      "train -- loss:178.808, prate:0.788, x_loss:4.72284, z_loss:174.085, anchor_loss:0\n",
      "eval -- loss:1403.62, prate:0.794, x_loss:8.01497, z_loss:46.209, alt_x_loss:1357.41, anchor_loss:0.148629\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:1984.42, prate:0.788, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:796.508, prate:0.785, x_loss:648.922, z_loss:47.4478, alt_x_loss:749.06, anchor_loss:0.152284\n",
      "epoch:\t1\n",
      "train -- loss:1197.99, prate:0.767, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:564.798, prate:0.771, x_loss:795.738, z_loss:48.3671, alt_x_loss:516.431, anchor_loss:0.162006\n",
      "epoch:\t2\n",
      "train -- loss:1552.72, prate:0.763, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:437.328, prate:0.76, x_loss:1328.49, z_loss:47.9223, alt_x_loss:389.406, anchor_loss:0.162108\n",
      "epoch:\t3\n",
      "train -- loss:819.026, prate:0.755, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:357.908, prate:0.751, x_loss:1652.66, z_loss:52.7137, alt_x_loss:305.194, anchor_loss:0.162031\n",
      "epoch:\t4\n",
      "train -- loss:1077.82, prate:0.764, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:338.004, prate:0.743, x_loss:1697.22, z_loss:47.9214, alt_x_loss:290.082, anchor_loss:0.159658\n",
      "epoch:\t5\n",
      "train -- loss:533.894, prate:0.736, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:324.64, prate:0.738, x_loss:1730.44, z_loss:46.2491, alt_x_loss:278.391, anchor_loss:0.159148\n",
      "epoch:\t6\n",
      "train -- loss:1086.11, prate:0.727, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:320.292, prate:0.73, x_loss:1788.22, z_loss:53.9306, alt_x_loss:266.362, anchor_loss:0.160524\n",
      "epoch:\t7\n",
      "train -- loss:861.919, prate:0.736, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:275.076, prate:0.725, x_loss:1930.37, z_loss:49.1624, alt_x_loss:225.913, anchor_loss:0.161044\n",
      "epoch:\t8\n",
      "train -- loss:982.885, prate:0.707, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:277.365, prate:0.719, x_loss:2022.02, z_loss:48.7452, alt_x_loss:228.62, anchor_loss:0.162119\n",
      "epoch:\t9\n",
      "train -- loss:648.462, prate:0.719, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:268.596, prate:0.712, x_loss:2172.09, z_loss:48.7567, alt_x_loss:219.839, anchor_loss:0.163343\n",
      "Final val losses: {'passing_rate': 0.7124750018119812, 'x_loss': 2172.0859375, 'z_loss': 48.7567253112793, 'alt_x_loss': 219.8389434814453, 'encoder_anchor_loss': 0.06838412582874298, 'decoder_anchor_loss': 0.09495899081230164, 'loss': 268.5956687927246}\n",
      "Saved model weights to ./swae-lamb=1.pkl\n",
      "Passing rate of z_decoded 0.714425\n",
      "Passing rate of x_reconstructed 0.79155\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.714425\n",
      "Passing rate of x_reconstructed 0.79155\n",
      "\n",
      "\n",
      "Training with lamb=10\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 10, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:252761, prate:0.673, x_loss:8012.06, z_loss:24474.9, anchor_loss:0\n",
      "eval -- loss:68420.2, prate:0.67, x_loss:5875.34, z_loss:30285.7, alt_x_loss:38134.5, anchor_loss:0.500895\n",
      "epoch:\t100\n",
      "train -- loss:1813.06, prate:0.706, x_loss:129.517, z_loss:168.354, anchor_loss:0\n",
      "eval -- loss:707.966, prate:0.69, x_loss:105.252, z_loss:63.1181, alt_x_loss:644.848, anchor_loss:0.122989\n",
      "epoch:\t200\n",
      "train -- loss:2471.48, prate:0.67, x_loss:80.423, z_loss:239.106, anchor_loss:0\n",
      "eval -- loss:526.238, prate:0.697, x_loss:83.772, z_loss:51.4291, alt_x_loss:474.809, anchor_loss:0.124855\n",
      "epoch:\t300\n",
      "train -- loss:1292.95, prate:0.682, x_loss:57.6047, z_loss:123.535, anchor_loss:0\n",
      "eval -- loss:705.303, prate:0.695, x_loss:62.3449, z_loss:51.1953, alt_x_loss:654.108, anchor_loss:0.142149\n",
      "epoch:\t400\n",
      "train -- loss:2128.45, prate:0.672, x_loss:49.9491, z_loss:207.85, anchor_loss:0\n",
      "eval -- loss:555.064, prate:0.682, x_loss:87.3419, z_loss:47.942, alt_x_loss:507.122, anchor_loss:0.160685\n",
      "epoch:\t500\n",
      "train -- loss:1863.77, prate:0.658, x_loss:40.0254, z_loss:182.375, anchor_loss:0\n",
      "eval -- loss:734.884, prate:0.672, x_loss:46.6207, z_loss:47.2332, alt_x_loss:687.651, anchor_loss:0.172341\n",
      "epoch:\t600\n",
      "train -- loss:1811.04, prate:0.667, x_loss:46.1033, z_loss:176.494, anchor_loss:0\n",
      "eval -- loss:551.629, prate:0.664, x_loss:38.8, z_loss:42.7008, alt_x_loss:508.928, anchor_loss:0.170384\n",
      "epoch:\t700\n",
      "train -- loss:1354.18, prate:0.636, x_loss:58.0618, z_loss:129.612, anchor_loss:0\n",
      "eval -- loss:442.881, prate:0.649, x_loss:50.0364, z_loss:45.1312, alt_x_loss:397.75, anchor_loss:0.168997\n",
      "epoch:\t800\n",
      "train -- loss:1608.09, prate:0.645, x_loss:44.2215, z_loss:156.387, anchor_loss:0\n",
      "eval -- loss:471.68, prate:0.646, x_loss:37.4815, z_loss:54.8158, alt_x_loss:416.864, anchor_loss:0.167894\n",
      "epoch:\t900\n",
      "train -- loss:1140.12, prate:0.634, x_loss:47.9384, z_loss:109.218, anchor_loss:0\n",
      "eval -- loss:463.23, prate:0.639, x_loss:33.22, z_loss:43.9574, alt_x_loss:419.272, anchor_loss:0.158895\n",
      "epoch:\t999\n",
      "train -- loss:1565.9, prate:0.643, x_loss:46.1624, z_loss:151.974, anchor_loss:0\n",
      "eval -- loss:389.573, prate:0.639, x_loss:42.5269, z_loss:43.63, alt_x_loss:345.943, anchor_loss:0.158403\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:1238.7, prate:0.628, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:338.908, prate:0.631, x_loss:108.097, z_loss:44.5094, alt_x_loss:294.398, anchor_loss:0.159986\n",
      "epoch:\t1\n",
      "train -- loss:1145.44, prate:0.603, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:325.041, prate:0.614, x_loss:198.899, z_loss:45.127, alt_x_loss:279.914, anchor_loss:0.164465\n",
      "epoch:\t2\n",
      "train -- loss:1744.99, prate:0.619, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:288.706, prate:0.597, x_loss:368.089, z_loss:44.9907, alt_x_loss:243.715, anchor_loss:0.170059\n",
      "epoch:\t3\n",
      "train -- loss:777.247, prate:0.576, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:262.907, prate:0.58, x_loss:594.456, z_loss:50.0436, alt_x_loss:212.864, anchor_loss:0.176693\n",
      "epoch:\t4\n",
      "train -- loss:1106.91, prate:0.574, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:225.414, prate:0.569, x_loss:852.478, z_loss:46.2197, alt_x_loss:179.195, anchor_loss:0.181556\n",
      "epoch:\t5\n",
      "train -- loss:1015.35, prate:0.548, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:275.721, prate:0.558, x_loss:975.673, z_loss:44.7054, alt_x_loss:231.016, anchor_loss:0.186925\n",
      "epoch:\t6\n",
      "train -- loss:844.649, prate:0.536, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:281.2, prate:0.548, x_loss:1035.85, z_loss:51.2541, alt_x_loss:229.946, anchor_loss:0.19127\n",
      "epoch:\t7\n",
      "train -- loss:655.221, prate:0.528, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:248.507, prate:0.538, x_loss:1164.14, z_loss:47.6254, alt_x_loss:200.881, anchor_loss:0.196526\n",
      "epoch:\t8\n",
      "train -- loss:472.276, prate:0.516, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:230.451, prate:0.529, x_loss:1251.25, z_loss:46.1261, alt_x_loss:184.325, anchor_loss:0.200709\n",
      "epoch:\t9\n",
      "train -- loss:865.448, prate:0.542, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:259.034, prate:0.522, x_loss:1291.31, z_loss:47.3061, alt_x_loss:211.727, anchor_loss:0.204091\n",
      "Final val losses: {'passing_rate': 0.5217999815940857, 'x_loss': 1291.3070068359375, 'z_loss': 47.30613708496094, 'alt_x_loss': 211.7274627685547, 'encoder_anchor_loss': 0.056512102484703064, 'decoder_anchor_loss': 0.14757852256298065, 'loss': 259.0335998535156}\n",
      "Saved model weights to ./swae-lamb=10.pkl\n",
      "Passing rate of z_decoded 0.52205\n",
      "Passing rate of x_reconstructed 0.835625\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.52205\n",
      "Passing rate of x_reconstructed 0.835625\n",
      "\n",
      "\n",
      "Training with lamb=100\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 100, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:2.44296e+06, prate:0.674, x_loss:8577.67, z_loss:24343.8, anchor_loss:0\n",
      "eval -- loss:68293.9, prate:0.67, x_loss:6342.77, z_loss:30153.1, alt_x_loss:38140.7, anchor_loss:0.514252\n",
      "epoch:\t100\n",
      "train -- loss:17691.6, prate:0.666, x_loss:548.747, z_loss:171.429, anchor_loss:0\n",
      "eval -- loss:18494.3, prate:0.665, x_loss:804.16, z_loss:76.0156, alt_x_loss:18418.3, anchor_loss:0.718173\n",
      "epoch:\t200\n",
      "train -- loss:23365.3, prate:0.669, x_loss:343.752, z_loss:230.216, anchor_loss:0\n",
      "eval -- loss:58544.2, prate:0.661, x_loss:1838.44, z_loss:53.1804, alt_x_loss:58491, anchor_loss:0.726502\n",
      "epoch:\t300\n",
      "train -- loss:12042.1, prate:0.635, x_loss:373.137, z_loss:116.69, anchor_loss:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval -- loss:66880.4, prate:0.643, x_loss:3078.57, z_loss:57.831, alt_x_loss:66822.6, anchor_loss:0.782034\n",
      "epoch:\t400\n",
      "train -- loss:19674.3, prate:0.647, x_loss:227.626, z_loss:194.467, anchor_loss:0\n",
      "eval -- loss:69060.6, prate:0.641, x_loss:3796.38, z_loss:49.9916, alt_x_loss:69010.6, anchor_loss:0.786674\n",
      "epoch:\t500\n",
      "train -- loss:19083.7, prate:0.668, x_loss:168.247, z_loss:189.154, anchor_loss:0\n",
      "eval -- loss:63618.3, prate:0.654, x_loss:4984.55, z_loss:52.5487, alt_x_loss:63565.7, anchor_loss:0.779341\n",
      "epoch:\t600\n",
      "train -- loss:19005.6, prate:0.693, x_loss:206.623, z_loss:187.989, anchor_loss:0\n",
      "eval -- loss:67263.1, prate:0.667, x_loss:8450.16, z_loss:43.1745, alt_x_loss:67219.9, anchor_loss:0.761896\n",
      "epoch:\t700\n",
      "train -- loss:13686.3, prate:0.685, x_loss:151.279, z_loss:135.351, anchor_loss:0\n",
      "eval -- loss:58315.9, prate:0.674, x_loss:8199.62, z_loss:48.4401, alt_x_loss:58267.5, anchor_loss:0.711092\n",
      "epoch:\t800\n",
      "train -- loss:12907.3, prate:0.692, x_loss:166.788, z_loss:127.405, anchor_loss:0\n",
      "eval -- loss:51952, prate:0.675, x_loss:9466.54, z_loss:53.4674, alt_x_loss:51898.6, anchor_loss:0.692819\n",
      "epoch:\t900\n",
      "train -- loss:12366.8, prate:0.694, x_loss:155.704, z_loss:122.111, anchor_loss:0\n",
      "eval -- loss:45467.5, prate:0.671, x_loss:8744.26, z_loss:46.8915, alt_x_loss:45420.6, anchor_loss:0.692324\n",
      "epoch:\t999\n",
      "train -- loss:13523.1, prate:0.696, x_loss:153.531, z_loss:133.695, anchor_loss:0\n",
      "eval -- loss:43382.8, prate:0.679, x_loss:9019.19, z_loss:51.8546, alt_x_loss:43331, anchor_loss:0.692501\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:4600.37, prate:0.688, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:2716.01, prate:0.682, x_loss:25865.1, z_loss:48.8872, alt_x_loss:2667.12, anchor_loss:0.623912\n",
      "epoch:\t1\n",
      "train -- loss:2948.33, prate:0.688, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:2079.86, prate:0.681, x_loss:36882.3, z_loss:50.5837, alt_x_loss:2029.27, anchor_loss:0.580803\n",
      "epoch:\t2\n",
      "train -- loss:1444.74, prate:0.666, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:1071.77, prate:0.667, x_loss:33679, z_loss:47.7953, alt_x_loss:1023.98, anchor_loss:0.572425\n",
      "epoch:\t3\n",
      "train -- loss:1057.88, prate:0.68, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:905.53, prate:0.658, x_loss:30854.4, z_loss:53.9349, alt_x_loss:851.595, anchor_loss:0.56944\n",
      "epoch:\t4\n",
      "train -- loss:1187.24, prate:0.642, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:895.824, prate:0.649, x_loss:31480.8, z_loss:46.0264, alt_x_loss:849.797, anchor_loss:0.567386\n",
      "epoch:\t5\n",
      "train -- loss:831.665, prate:0.643, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:737.453, prate:0.643, x_loss:32957.1, z_loss:48.9442, alt_x_loss:688.509, anchor_loss:0.56545\n",
      "epoch:\t6\n",
      "train -- loss:1249.92, prate:0.648, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:742.887, prate:0.637, x_loss:33528.3, z_loss:52.9667, alt_x_loss:689.92, anchor_loss:0.568463\n",
      "epoch:\t7\n",
      "train -- loss:1573.69, prate:0.621, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:719.215, prate:0.63, x_loss:33734.8, z_loss:50.7892, alt_x_loss:668.426, anchor_loss:0.570736\n",
      "epoch:\t8\n",
      "train -- loss:1197.37, prate:0.632, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:689.16, prate:0.625, x_loss:33774.8, z_loss:50.0433, alt_x_loss:639.117, anchor_loss:0.57183\n",
      "epoch:\t9\n",
      "train -- loss:1518.67, prate:0.627, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:616.526, prate:0.619, x_loss:34033.7, z_loss:50.4394, alt_x_loss:566.087, anchor_loss:0.575014\n",
      "Final val losses: {'passing_rate': 0.6190500259399414, 'x_loss': 34033.734375, 'z_loss': 50.43937301635742, 'alt_x_loss': 566.0870361328125, 'encoder_anchor_loss': 0.19805067777633667, 'decoder_anchor_loss': 0.37696361541748047, 'loss': 616.5264091491699}\n",
      "Saved model weights to ./swae-lamb=100.pkl\n",
      "Passing rate of z_decoded 0.6178\n",
      "Passing rate of x_reconstructed 0.823525\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.6178\n",
      "Passing rate of x_reconstructed 0.823525\n",
      "\n",
      "\n",
      "Training with lamb=1000\n",
      "{'lr': 0.001, 'beta': 1.0, 'lamb': 1000, 'tau': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'epochs': 1000, 'log_freq': 100}\n",
      "epoch:\t0\n",
      "train -- loss:2.43459e+07, prate:0.673, x_loss:8686.94, z_loss:24337.3, anchor_loss:0\n",
      "eval -- loss:68346.4, prate:0.67, x_loss:6402.62, z_loss:30158, alt_x_loss:38188.4, anchor_loss:0.515165\n",
      "epoch:\t100\n",
      "train -- loss:181822, prate:0.753, x_loss:2678.06, z_loss:179.144, anchor_loss:0\n",
      "eval -- loss:6764.6, prate:0.753, x_loss:3499.72, z_loss:77.4008, alt_x_loss:6687.2, anchor_loss:0.825051\n",
      "epoch:\t200\n",
      "train -- loss:210666, prate:0.708, x_loss:1487.44, z_loss:209.179, anchor_loss:0\n",
      "eval -- loss:81180.7, prate:0.713, x_loss:18358.7, z_loss:51.4797, alt_x_loss:81129.2, anchor_loss:0.885801\n",
      "epoch:\t300\n",
      "train -- loss:123347, prate:0.69, x_loss:1500.4, z_loss:121.846, anchor_loss:0\n",
      "eval -- loss:87964.5, prate:0.665, x_loss:30250.8, z_loss:60.8995, alt_x_loss:87903.6, anchor_loss:0.919562\n",
      "epoch:\t400\n",
      "train -- loss:196176, prate:0.66, x_loss:1017.88, z_loss:195.158, anchor_loss:0\n",
      "eval -- loss:88507.6, prate:0.624, x_loss:34658.8, z_loss:48.806, alt_x_loss:88458.8, anchor_loss:0.953526\n",
      "epoch:\t500\n",
      "train -- loss:169563, prate:0.64, x_loss:958.666, z_loss:168.605, anchor_loss:0\n",
      "eval -- loss:85777.5, prate:0.611, x_loss:39944.5, z_loss:47.6364, alt_x_loss:85729.9, anchor_loss:0.962204\n",
      "epoch:\t600\n",
      "train -- loss:178131, prate:0.658, x_loss:979.884, z_loss:177.151, anchor_loss:0\n",
      "eval -- loss:80925.8, prate:0.623, x_loss:33786.1, z_loss:39.3851, alt_x_loss:80886.4, anchor_loss:0.917614\n",
      "epoch:\t700\n",
      "train -- loss:131567, prate:0.663, x_loss:847.069, z_loss:130.72, anchor_loss:0\n",
      "eval -- loss:85728.5, prate:0.635, x_loss:38461.8, z_loss:41.6836, alt_x_loss:85686.8, anchor_loss:0.813581\n",
      "epoch:\t800\n",
      "train -- loss:129278, prate:0.661, x_loss:923.581, z_loss:128.354, anchor_loss:0\n",
      "eval -- loss:87626, prate:0.64, x_loss:39678.2, z_loss:49.6019, alt_x_loss:87576.4, anchor_loss:0.840427\n",
      "epoch:\t900\n",
      "train -- loss:126765, prate:0.675, x_loss:744.413, z_loss:126.02, anchor_loss:0\n",
      "eval -- loss:81400, prate:0.643, x_loss:35837.6, z_loss:39.9647, alt_x_loss:81360, anchor_loss:0.854204\n",
      "epoch:\t999\n",
      "train -- loss:132583, prate:0.649, x_loss:690.266, z_loss:131.893, anchor_loss:0\n",
      "eval -- loss:78284.6, prate:0.65, x_loss:33809.2, z_loss:47.4823, alt_x_loss:78237.1, anchor_loss:0.82151\n",
      "{'beta': 0, 'tau': 1, 'lamb': 0, 'rho': 0, 'nu_e': 0, 'nu_d': 0, 'lr': 0.0001, 'epochs': 10, 'log_freq': 1}\n",
      "epoch:\t0\n",
      "train -- loss:8267.33, prate:0.656, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:6067.39, prate:0.652, x_loss:60125.6, z_loss:46.5461, alt_x_loss:6020.84, anchor_loss:0.729058\n",
      "epoch:\t1\n",
      "train -- loss:2578.13, prate:0.65, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:2139.28, prate:0.642, x_loss:73918.5, z_loss:47.735, alt_x_loss:2091.54, anchor_loss:0.703467\n",
      "epoch:\t2\n",
      "train -- loss:1301.71, prate:0.651, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:1611.05, prate:0.628, x_loss:75588.6, z_loss:46.7736, alt_x_loss:1564.28, anchor_loss:0.721649\n",
      "epoch:\t3\n",
      "train -- loss:1539.46, prate:0.633, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:1076.67, prate:0.619, x_loss:75381.5, z_loss:54.952, alt_x_loss:1021.72, anchor_loss:0.755058\n",
      "epoch:\t4\n",
      "train -- loss:1389.86, prate:0.616, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:899.199, prate:0.611, x_loss:75840.4, z_loss:47.463, alt_x_loss:851.736, anchor_loss:0.78137\n",
      "epoch:\t5\n",
      "train -- loss:919.5, prate:0.602, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:772.092, prate:0.606, x_loss:77405.8, z_loss:46.9315, alt_x_loss:725.161, anchor_loss:0.784038\n",
      "epoch:\t6\n",
      "train -- loss:1153.56, prate:0.609, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:819.806, prate:0.602, x_loss:78950.7, z_loss:53.0889, alt_x_loss:766.717, anchor_loss:0.785765\n",
      "epoch:\t7\n",
      "train -- loss:1318.55, prate:0.585, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:752.4, prate:0.598, x_loss:79526.9, z_loss:49.7236, alt_x_loss:702.676, anchor_loss:0.783619\n",
      "epoch:\t8\n",
      "train -- loss:1409.94, prate:0.589, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:733.637, prate:0.595, x_loss:81054.6, z_loss:46.7104, alt_x_loss:686.927, anchor_loss:0.787953\n",
      "epoch:\t9\n",
      "train -- loss:1159.47, prate:0.601, x_loss:0, z_loss:0, anchor_loss:0\n",
      "eval -- loss:700.604, prate:0.591, x_loss:80640.1, z_loss:48.7019, alt_x_loss:651.902, anchor_loss:0.79159\n",
      "Final val losses: {'passing_rate': 0.5907750129699707, 'x_loss': 80640.078125, 'z_loss': 48.701934814453125, 'alt_x_loss': 651.901611328125, 'encoder_anchor_loss': 0.2912024259567261, 'decoder_anchor_loss': 0.5003876090049744, 'loss': 700.6035461425781}\n",
      "Saved model weights to ./swae-lamb=1000.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing rate of z_decoded 0.590725\n",
      "Passing rate of x_reconstructed 0.689375\n",
      "Passing rate of x 0.99145\n",
      "Passing rate of z_decoded 0.590725\n",
      "Passing rate of x_reconstructed 0.689375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = '.'\n",
    "verbose = True\n",
    "lambs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for lamb in lambs:\n",
    "    joint_step_config['lamb'] = lamb\n",
    "    print(f'Training with lamb={lamb}')\n",
    "\n",
    "    # Reset seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create a new model and optimizer\n",
    "    model = Autoencoder(x_dim, z_dim, ConditionalModel=StochasticResNet, encoder_hidden_layer_dims=hidden_layer_dims,\n",
    "                    stoch_enc=True, stoch_dec=True, activation=activation, raw_io=raw_io,\n",
    "                    x_inv_masses=x_inv_masses, x_stats=np.stack([x_train_mean, x_train_std]),\n",
    "                    z_inv_masses=z_inv_masses, z_stats=np.stack([z_train_mean, z_train_std]),\n",
    "                    # ResNet settings:\n",
    "                    io_residual=True,\n",
    "                    res_mlp_depth=2\n",
    "                            )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = {}\n",
    "    # Joint SWAE training\n",
    "    log_freq = joint_step_config['log_freq']\n",
    "    eval_losses, history = train_and_val(model, train_loaders, eval_loaders, joint_step_config, optimizer, verbose=verbose, prev_hist=history, log_freq=log_freq)\n",
    "    \n",
    "    # Decoder fine-tuning with SWD(p(x), p_D(x)) loss only\n",
    "    ## Reduce lr\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = decoder_finetuning_config[\"lr\"]\n",
    "\n",
    "    log_freq = decoder_finetuning_config['log_freq']\n",
    "    eval_losses, history = train_and_val(model, train_loaders, eval_loaders, decoder_finetuning_config, optimizer, verbose=verbose, prev_hist=history, log_freq=log_freq)\n",
    "    \n",
    "    print('Final val losses:', eval_losses)\n",
    "    \n",
    "    # Save history in JSON-lines format\n",
    "    ## convert pytorch float tensors into plain numpy float arrs in history\n",
    "    for key, val in history.items():\n",
    "        if isinstance(val, (list, np.ndarray)) :\n",
    "            if not isinstance(np.sum(val), (int, np.integer)):  # my crude test to see if this is an array of float type\n",
    "                history[key] = [float(n) for n in val]\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_json(os.path.join(save_dir, f'history-lamb={lamb}.jsonl'), orient='records', lines=True)\n",
    "    \n",
    "    \n",
    "    # Save trained model\n",
    "    save_path = os.path.join(save_dir, f'swae-lamb={lamb}.pkl')\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print('Saved model weights to', save_path)\n",
    "    \n",
    "    \n",
    "    # Evaluate and plot results\n",
    "    from plot_utils import plotFunction\n",
    "    from ppttbar_constraints import threshold_check\n",
    "    #-- Reset random seeds --#\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #-- Convert model to run on CPU and run on validation data --#\n",
    "    model.to('cpu')\n",
    "    model.encoder.output_stats.to('cpu')\n",
    "    model.decoder.output_stats.to('cpu')\n",
    "\n",
    "    all_arrs = {'train': {}, 'val': {}}  # This will store all numpy arrays of interest\n",
    "    all_arrs['train']['x'] = x_train\n",
    "    all_arrs['train']['z'] = z_train\n",
    "    all_arrs['val']['x']   = x_val\n",
    "    all_arrs['val']['z']   = z_val\n",
    "\n",
    "    for data_key in 'train', 'val':\n",
    "        arrs = all_arrs[data_key]\n",
    "        arrs['z_decoded']       = model.decode(torch.from_numpy(arrs['z'])) # p_D(x) = \\int_z p(z) p_D(x|z)  \"x_pred_truth\"\n",
    "        arrs['x_encoded']       = model.encode(torch.from_numpy(arrs['x'])) # p_E(z) = \\int_x p(x) p_E(z|x)  \"z_pred\"\n",
    "        arrs['x_reconstructed'] = model.decode(arrs['x_encoded'])           # p_D(y) = \\int_x \\int_z p(x) p_E(z|x) p_D(y|z) \"x_pred\"\n",
    "\n",
    "        # Feed the same z input to the decoder multiple times and study the stochastic of the output\n",
    "        num_repeats = 100\n",
    "        num_diff_zs = 100\n",
    "        arrs['z_rep']         = np.array([np.repeat(arrs['z'][i:i+1], num_repeats, axis=0) for i in range(num_diff_zs)]) # \"z_fixed\"\n",
    "        z_rep_tensor          = torch.from_numpy(arrs['z_rep'])                                                          # tmp\n",
    "        arrs['z_decoded_rep'] = np.array([model.decode(z_rep_tensor[i]).detach().numpy() for i in range(num_diff_zs)])   # \"x_pred_truth_fixed\"\n",
    "        arrs['x_rep']         = np.array([np.repeat(arrs['x'][i:i+1], num_repeats, axis=0) for i in range(num_diff_zs)]) # \"x_fixed\"\n",
    "\n",
    "        # Convert all results to numpy arrays\n",
    "        for (field, arr) in arrs.items():\n",
    "            if isinstance(arr, torch.Tensor):\n",
    "                arrs[field] = arr.detach().numpy()\n",
    "                \n",
    "    # Create new arrays from model output that passes cuts\n",
    "    ## Just do it on validation set since we only record results on val set\n",
    "#     for data_key in ['train', 'val']:\n",
    "    for data_key in ['val']:\n",
    "        arrs = all_arrs[data_key]\n",
    "        for field in ('z_decoded', 'x_reconstructed'):\n",
    "            arr = arrs[field]\n",
    "\n",
    "            if raw_io:\n",
    "                arr_raw = arr\n",
    "            else:\n",
    "                arr_raw = (arr * x_train_std) + x_train_mean\n",
    "\n",
    "            # Keep only events that pass threshold constraint\n",
    "            good_mask = threshold_check(arr_raw)\n",
    "            if verbose:\n",
    "                print('Passing rate of', field, good_mask.mean())\n",
    "            arr_raw = arr_raw[good_mask] \n",
    "\n",
    "            if raw_io:\n",
    "                arr = arr_raw\n",
    "            else:\n",
    "                arr = (arr_raw - x_train_mean) / x_train_std\n",
    "\n",
    "            arrs[field+'_'] = arr\n",
    "\n",
    "            if field == 'z_decoded':\n",
    "                arrs['z_decoded_good_mask'] = good_mask\n",
    "                arrs['z_'] = arrs['z'][good_mask]\n",
    "            else:\n",
    "                arrs['x_encoded_'] = arrs['x_encoded'][good_mask]\n",
    "                \n",
    "    data_key = 'val'\n",
    "    arrs = all_arrs[data_key]\n",
    "    ## Set overall plotting limits\n",
    "    if raw_io:\n",
    "        x_display_lims = [\n",
    "            [(-250, 250), (-250, 250), (-700, 700), (0, 700)],\n",
    "            [(-250, 250), (-250, 250), (-2900, 2900), (0, 2900)],\n",
    "            [(-300, 300), (-300, 300), (-700, 700), (0, 700)],\n",
    "            [(-250, 250), (-250, 250), (-700, 700), (0, 700)],\n",
    "            [(-200, 200), (-200, 200), (-700, 700), (0, 700)],\n",
    "            [(-100, 100), (-100, 100), (-700, 700), (0, 700)]\n",
    "        ]\n",
    "        z_display_lims = [\n",
    "            [(-250, 250), (-250, 250), (-700, 700), (0, 700)],\n",
    "            [(-250, 250), (-250, 250), (-1000, 1000), (0, 1000)], # Only differs from x_display_lims here\n",
    "            [(-300, 300), (-300, 300), (-700, 700), (0, 700)],\n",
    "            [(-250, 250), (-250, 250), (-700, 700), (0, 700)],\n",
    "            [(-200, 200), (-200, 200), (-700, 700), (0, 700)],\n",
    "            [(-100, 100), (-100, 100), (-700, 700), (0, 700)]\n",
    "        ]\n",
    "    else:  # Standardized data\n",
    "        x_display_lims = z_display_lims = [[(-2.5,2.5),(-2.5,2.5),(-5,5),(-2,5)] for _ in range(6)]\n",
    "        \n",
    "    ## Z marginals\n",
    "    dataList = [arrs['z'], arrs['x_encoded']]\n",
    "    pltDim   = (6,4)\n",
    "    numBins  = 50\n",
    "    binsList = []\n",
    "    for i in range(pltDim[0]):\n",
    "        for j in range(pltDim[1]):\n",
    "\n",
    "            low  = z_display_lims[i][j][0]\n",
    "            high = z_display_lims[i][j][1]\n",
    "\n",
    "            binsList.append(np.linspace(low, high, numBins))\n",
    "\n",
    "    particleNameList = [r'$e^-$', r'$\\bar{\\nu}_e$', r'$b$', r'$\\bar{b}$', r'$u$', r'$\\bar{d}$']\n",
    "    fig = plotFunction(dataList = dataList, pltDim = pltDim, binsList = binsList, particleNameList = particleNameList, show=False)\n",
    "    fig.savefig(os.path.join(save_dir, f'Z_marginals-lamb={lamb}.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    ## X marginals\n",
    "    dataList = [arrs['x'], arrs['x_reconstructed_'], arrs['z_decoded_']] # Use only passing events\n",
    "    pltDim   = (6,4)\n",
    "    numBins  = 50\n",
    "    binsList = []\n",
    "    for i in range(pltDim[0]):\n",
    "        for j in range(pltDim[1]):\n",
    "\n",
    "            low  = x_display_lims[i][j][0]\n",
    "            high = x_display_lims[i][j][1]\n",
    "\n",
    "            binsList.append(np.linspace(low, high, numBins))\n",
    "\n",
    "    particleNameList = [r'$e^-$', r'$MET$', r'$Jet1$', r'$Jet2$', r'$Jet3$', r'$Jet4$']\n",
    "    fig = plotFunction(dataList = dataList, pltDim = pltDim, binsList = binsList, particleNameList = particleNameList, show=False)\n",
    "    fig.savefig(os.path.join(save_dir, f'X_marginals-lamb={lamb}.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Evaluate derived quantities\n",
    "    from top_masses import ttbar_masses   \n",
    "    for field in 'x', 'z_decoded', 'x_reconstructed':\n",
    "\n",
    "        # Check if this has been calculated for x already \n",
    "        if field == 'x' and isinstance(arrs.get(field+'_masses'), np.ndarray): \n",
    "            continue\n",
    "\n",
    "        arr = arrs[field]\n",
    "\n",
    "        if raw_io:\n",
    "            arr_raw = arr\n",
    "        else:\n",
    "            arr_raw = (arr * x_train_std) + x_train_mean\n",
    "\n",
    "        # Keep only events that pass threshold constraint\n",
    "        good_mask = threshold_check(arr_raw)\n",
    "        if verbose:\n",
    "            print('Passing rate of', field, good_mask.mean())\n",
    "        arr_raw = arr_raw[good_mask]\n",
    "\n",
    "        # Calculate invariant mass derived quantities\n",
    "        masses = []\n",
    "        mass_keys = 'mttbar', 'mwlep', 'mwhad', 'mtoplep', 'mtophad'\n",
    "        for x in arr_raw:\n",
    "            mttbar, mwlep, mwhad, mtoplep, mtophad = ttbar_masses(list(x))\n",
    "            masses.append(np.array([mttbar, mwlep, mwhad, mtoplep, mtophad]))\n",
    "        masses = np.asarray(masses)\n",
    "        arrs[field+'_masses'] = masses\n",
    "    # Plot derived quantities\n",
    "    dataList = [arrs['x_masses'], arrs['x_reconstructed_masses'],arrs['z_decoded_masses']] \n",
    "    pltDim   = (2,4)\n",
    "    numBins  = 50\n",
    "    mass_lims = [(0,1500), (78, 175), (0,250), (50,350), (50,800)]\n",
    "    binsList = []\n",
    "    for i in range(len(mass_lims)):\n",
    "        low  = mass_lims[i][0]\n",
    "        high = mass_lims[i][1]\n",
    "\n",
    "        binsList.append(np.linspace(low, high, numBins))\n",
    "    particleNameList = []\n",
    "    nameList = [r'$M_{t \\bar{t}}$', r'$M_{W, leptonic}$', r'$M_{W, hadronic}$', r'$M_{t, leptonic}$', r'$M_{t, hadronic}$']\n",
    "\n",
    "    # Create plot\n",
    "    fig = plotFunction(dataList = dataList, pltDim = pltDim, binsList=binsList, particleNameList=particleNameList, nameList=nameList, show=False)\n",
    "    fig.savefig(os.path.join(save_dir, f'X_derived-lamb={lamb}.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Transport plots\n",
    "    from plot_utils import fullTransportPlot\n",
    "    nzList    = np.repeat(20,24).tolist()\n",
    "    nxList    = nzList\n",
    "    # z_display_lims and x_display_lims defined above\n",
    "    limzList = sum(z_display_lims, [])\n",
    "    limxList  = sum(x_display_lims, [])\n",
    "    pltDim    = (6,4)\n",
    "    titleList = [r'$p_x$',r'$p_y$',r'$p_z$',r'$E$','','','','','','','','','','','','','','','','','','','','']\n",
    "    fig = fullTransportPlot(arrs['z_'], arrs['z_decoded_'], nzList=nzList, nxList=nxList, limzList=limzList, limxList=limxList, pltDim=pltDim, titleList=titleList, show=False)\n",
    "    fig.savefig(os.path.join(save_dir, f'z-z_decoded-transport-lamb={lamb}.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    fig = fullTransportPlot(arrs['x_encoded_'], arrs['x_reconstructed_'], nzList=nzList, nxList=nxList, limzList=limzList, limxList=limxList, pltDim=pltDim, titleList=titleList, show=False)\n",
    "    fig.savefig(os.path.join(save_dir, f'x_encoded-x_reconstructed-transport-lamb={lamb}.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
