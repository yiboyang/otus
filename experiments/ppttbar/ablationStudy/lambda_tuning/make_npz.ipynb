{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTUS |  $ p p > t \\bar{t} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook obtains the npz file of the trained model's results on validation data for the ablation study. Below are details about the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks applies OTUS to our second test case: semi-leptonic $t \\bar{t}$ decay.\n",
    "\n",
    "Our physical latent-space is the $e^-$, $\\bar{\\nu}_e$, $b$, $\\bar{b}$, $u$, $\\bar{d}$ 4-momentum information produced by the program MadGraph.\n",
    "\n",
    "Our data-space data is the $e^+$, $MET$, $jet1$, $jet2$, $jet3$, $jet4$ 4-momentum information produced by the program Delphes. The jets are ordered in descending $p_T$.\n",
    "\n",
    "We arrange this information into 24 dimensional vectors\n",
    "\n",
    "- Latent space (z): [$p^{\\mu}_{e-}$,$p^{\\mu}_{\\bar{\\nu}_e}$,$p^{\\mu}_{b}$,$p^{\\mu}_{\\bar{b}}$,$p^{\\mu}_{u}$,$p^{\\mu}_{\\bar{d}}$]\n",
    "- Data space (x): [$p^{\\mu}_{e^-}$,$p^{\\mu}_{MET}$,$p^{\\mu}_{jet1}$,$p^{\\mu}_{jet2}$,$p^{\\mu}_{jet3}$,$p^{\\mu}_{jet4}$]\n",
    "\n",
    "where $p^{\\mu}=[p_x, p_y, p_z, E]$ is the 4-momentum of the given particle.\n",
    "\n",
    "###### Additional Losses and Constraints:\n",
    "We impose the following additional losses and constraints in this problem.\n",
    "\n",
    "As in the $p p > Z > e^+ e^-$ test case, we explicitly enforce the Minkowski metric in the output of the networks. Namely, the networks predict the 3-momenta ($\\vec{p}$) of the particles. Energy information is then restored using the Minkowski metric: $E^2 = |\\vec{p}|^2 + m^2$.\n",
    "\n",
    "We also explicitly enforce the lower $p_T$ threshold on jets, which requires that $p_T>20$ GeV. Only samples generated by the decoder which pass this threshold are used to calculate losses. This requires modifying the data-space loss term slightly. Additionally, to help with stable traiing, we choose a ResNet architecture for both our encoder and decoder networks.\n",
    "\n",
    "See the paper for more details: https://arxiv.org/abs/2101.08944."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "root_dir = '../../../../'\n",
    "\n",
    "#-- Add utilityFunctions/ to easily use utility .py files --#\n",
    "import sys\n",
    "sys.path.append(os.path.join(root_dir, \"utilityFunctions/\"))\n",
    "\n",
    "#-- Determine if using GPU or CPU --#\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # Set to '-1' to disable GPU\n",
    "from configs import device, data_dims\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb =  0.001\n"
     ]
    }
   ],
   "source": [
    "#-- Set appropriate lambda value --#\n",
    "# allLambs    = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "selectLambs = [0.001, 10, 100]\n",
    "lamb = selectLambs[0]\n",
    "print('lamb = ', lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data type:  float32\n"
     ]
    }
   ],
   "source": [
    "#-- Set directory short-cuts --#\n",
    "data_directory    = os.path.join(root_dir, \"data/\")\n",
    "dataset_name      = 'ppttbar'\n",
    "\n",
    "#-- Set random seeds --#\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#-- Raw or standardized inputs/outputs --#\n",
    "# If True, model inputs/outputs should be in the \"raw\" (unstandardized) space\n",
    "raw_io = True  \n",
    "\n",
    "#-- Set data type --#\n",
    "from configs import float_type\n",
    "print('Using data type: ', float_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Validation Data for Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data total shapes:  (262761, 24) (262761, 24)\n",
      "z_train shape, x_train shape:  (222761, 24) (222761, 24)\n",
      "z_val   shape, x_val   shape:  (40000, 24) (40000, 24)\n"
     ]
    }
   ],
   "source": [
    "from func_utils import get_dataset, standardize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#-- Get training and validation dataset --#\n",
    "dataset = get_dataset(dataset_name, data_dir=data_directory)\n",
    "z_data, x_data = dataset['z_data'], dataset['x_data']\n",
    "print(\"Data total shapes: \",z_data.shape, x_data.shape)\n",
    "\n",
    "x_dim = int(x_data.shape[1])\n",
    "z_dim = int(z_data.shape[1])\n",
    "\n",
    "#-- Split into training and validation sets --#\n",
    "train_size = 222761\n",
    "val_size = 40000  # Validation set used to evaluate/tune models\n",
    "\n",
    "x_train = x_data[:train_size, :]\n",
    "x_val = x_data[train_size:train_size+val_size, :]\n",
    "\n",
    "z_train = z_data[:train_size, :]\n",
    "z_val = z_data[train_size:train_size+val_size, :]\n",
    "\n",
    "#-- Convert data to proper type --#\n",
    "x_train, x_val, z_train, z_val = list(map(lambda x: x.astype(float_type), [x_train, x_val, z_train, z_val]))\n",
    "\n",
    "#-- Obtain mean and std information --#\n",
    "# This is needed to standardize/unstandardize data\n",
    "x_train_mean, x_train_std = np.mean(x_train, axis=0), np.std(x_train, axis=0)\n",
    "z_train_mean, z_train_std = np.mean(z_train, axis=0), np.std(z_train, axis=0)\n",
    "\n",
    "# If raw_io == False, then standardize the data with training set statistics\n",
    "if not raw_io:\n",
    "    x_train = (x_train - x_train_mean) / x_train_std  \n",
    "    x_val = (x_val - x_train_mean) / x_train_std  \n",
    "if not raw_io:\n",
    "    z_train = (z_train - z_train_mean) / z_train_std\n",
    "    z_val = (z_val - z_train_mean) / z_train_std\n",
    "\n",
    "#-- Set evaluation parameters --#\n",
    "eval_batch_size = 20000  # Always use high batch size on validation set to accurately assess performance\n",
    "eval_loaders = DataLoader(dataset=x_val, batch_size=eval_batch_size, shuffle=True), \\\n",
    "               DataLoader(dataset=z_val, batch_size=eval_batch_size, shuffle=True)\n",
    "\n",
    "print(\"z_train shape, x_train shape: \", z_train.shape, x_train.shape)\n",
    "print(\"z_val   shape, x_val   shape: \", z_val.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Define dictionary object for easy reference --#\n",
    "all_arrs = {'train': {}, 'val': {}}  # This will store all numpy arrays of interest\n",
    "all_arrs['train']['x'] = x_train\n",
    "all_arrs['train']['z'] = z_train\n",
    "all_arrs['val']['x']   = x_val\n",
    "all_arrs['val']['z']   = z_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target invariant masses (for both training and validation data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invariant mass relation: $m^2 = E^2 - |\\vec{p}|^2$. For objects with ill-defined mass (MET and Jets) we fix $m=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inv_masses = np.zeros(6)\n",
    "z_inv_masses = np.array([0., 0., 4.7, 4.7, 0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Specific Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from ppttbar_constraints import threshold_check\n",
    "from ppttbar_utils import train_and_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Meta Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoencoder, StochasticResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Latent loss function:\n",
    "Finite sample approximation of Sliced Wasserstein Distance (SWD) between $p(z)$ and $p_E(z) = \\int_x p(x) p_E(z|x)$\n",
    "\n",
    "- $L_{latent}(Z, \\tilde{Z}) = \\frac{1}{L * M} \\sum_{l=1}^{L} \\sum_{m=1}^{M} c((\\theta_l \\cdot z_m)_{sorted}, (\\theta_l \\cdot \\tilde{z}_m)_{sorted})$\n",
    "\n",
    "where $c(\\cdot, \\cdot) = |\\cdot - \\cdot|^2$\n",
    "\n",
    "###### Data loss function:\n",
    "- $L_{data}(X, \\tilde{X}) = \\frac{1}{M} \\sum_{m=1}^M [\\frac{1_S(\\tilde{x}_m)}{p_D(S)} \n",
    "c(x_m,  \\tilde{x}_m)]$\n",
    "\n",
    "where $c(\\cdot, \\cdot) = |\\cdot - \\cdot|^2$; $1_S(x)$ is the indicator function of $S$ so that it equals $1$ if $x \\in S$, and $0$ otherwise, and $p_D(S) := \\int dt p_D(t) 1_S(t)$ normalizes this distribution.\n",
    "\n",
    "###### Full loss function:\n",
    "- $L_{tot} = \\beta L_{data}(X, \\tilde{X}) + \\lambda L_{latent}(Z, \\tilde{Z})$ \n",
    "\n",
    "###### Core Hyperparameters\n",
    "The hyperparameter definitions are as follows:\n",
    "\n",
    "- num_hidden_layers: The number of hidden layers in both the encoder and decoder networks\n",
    "- dim_per_hidden_layer: The dimensions per hidden layer in both the encoder and decoder networks\n",
    "- lr: The learning rate of the networks\n",
    "- lamb: The $\\lambda$ coefficient in front of the latent loss term\n",
    "- num_slices: Number of random projections used for computing SWD\n",
    "- epochs: The number of epochs used during training\n",
    "\n",
    "Hyperparameters for other losses that were tried, but use during main training is currently discouraged:\n",
    "\n",
    "- tau: Coefficient in front of the alternate data-space loss (\"alt_x_loss\"), which is the SWD between $p(x)$ and $p_D(x):=\\int_z p(z) p_D(x|z)$\n",
    "- rho: Coefficient in front of an additional decoder constraint loss (based on soft-penalty approach to learning hard thresholds/ttbar_constraints)\n",
    "\n",
    "###### Joint Training Hyperparameters\n",
    "- beta: Coefficient in front of data loss, $L_{data}$ \n",
    "- beta_e: Coefficient in front of the encoder \"anchor loss\" \n",
    "- beta_d: Coefficient in front of the decoder \"anchor loss\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: most of the unspecified hyperparameters are set to 0 by default\n",
    "\n",
    "# common configs\n",
    "joint_step_config = {\n",
    "    'lr': 0.001,\n",
    "    'beta': 1.,  # coefficient in front of data loss, E[c(x, x reconstructed)], where c is typically the 2-norm.\n",
    "    'lamb': lamb,  # coefficient in front of latent loss, SWD between p(z) and Q(z):=\\int_x p(x) Q(z|x)\n",
    "    'tau': 0,  # coefficient in front of \"alt_x_loss\", which is the SWD between p(x) and p_G(x):=\\int_z p(z) p_G(x|z);\n",
    "               # this loss is not part of the original WAE formulation and is not used.\n",
    "    'rho': 0, # coef in front of decoder constraint loss (based on soft-penalty approach to learning hard thresholds/ttbar_constraints)\n",
    "    'nu_e': 0,  # coefficient in front of encoder \"anchor loss\"\n",
    "    'nu_d': 0,\n",
    "    'epochs': 1000,\n",
    "    'log_freq': 100,\n",
    "}\n",
    "\n",
    "\n",
    "decoder_finetuning_config = {\n",
    "    'beta': 0,  # coefficient in front of data loss in (S)WAE objective\n",
    "    'tau': 1, # coefficient in front of \"alt_x_loss\", which is the SWD between p(x) and p_D(x)\n",
    "    'lamb': 0, # disable latent loss\n",
    "    'rho': 0, # no x_constraint_loss (no longer used)\n",
    "    'nu_e': 0,  # anchor loss\n",
    "    'nu_d': 0,\n",
    "    'lr': 0.0001,  # reduced lr for fine-tuning\n",
    "    'epochs': 10,\n",
    "    'log_freq': 1,\n",
    "}\n",
    "\n",
    "\n",
    "hidden_layer_dims = [64, 64]\n",
    "activation = torch.nn.ReLU\n",
    "from models import Autoencoder, StochasticResNet\n",
    "model = Autoencoder(x_dim, z_dim, ConditionalModel=StochasticResNet, encoder_hidden_layer_dims=hidden_layer_dims,\n",
    "                    stoch_enc=True, stoch_dec=True, activation=activation, raw_io=raw_io,\n",
    "                    x_inv_masses=x_inv_masses, x_stats=np.stack([x_train_mean, x_train_std]),\n",
    "                    z_inv_masses=z_inv_masses, z_stats=np.stack([z_train_mean, z_train_std]),\n",
    "                    # ResNet settings:\n",
    "                    io_residual=True,\n",
    "                    res_mlp_depth=2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): StochasticResNet(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ResBlock(\n",
       "        (module): Sequential(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=64, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): StochasticResNet(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ResBlock(\n",
       "        (module): Sequential(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=64, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Trained Model on Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier downstream analysis we also evaluate the trained model on our testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): StochasticResNet(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ResBlock(\n",
       "        (module): Sequential(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=64, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): StochasticResNet(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ResBlock(\n",
       "        (module): Sequential(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=64, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Reset random seeds --#\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#-- Evaluate trained model on validation dataset --#\n",
    "\n",
    "# Use CPU instead of GPU\n",
    "model.to('cpu')\n",
    "model.encoder.output_stats.to('cpu')\n",
    "model.decoder.output_stats.to('cpu')\n",
    "\n",
    "#-- Set save directory location for npz files --#\n",
    "save_dir      = './npzFiles/'\n",
    "save_filename = f'swae-lamb={lamb}.npz'\n",
    "\n",
    "#-- Load model's trained weights and set to evaluation mode --#\n",
    "model.load_state_dict(torch.load(f'swae-lamb={lamb}.pkl', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 24) (40000, 24)\n"
     ]
    }
   ],
   "source": [
    "#-- Get validation dataset set into dictionary --#\n",
    "# z_val and x_val already defined from above\n",
    "print(all_arrs['val']['x'].shape, all_arrs['val']['z'].shape)\n",
    "\n",
    "# Evaluate trained model on validation dataset\n",
    "arrs = all_arrs['val']\n",
    "\n",
    "arrs['z_decoded'] = model.decode(torch.from_numpy(arrs['z'])) # p_D(x) = \\int_z p(z) p_D(x|z)  \"x_pred_truth\"\n",
    "arrs['x_encoded'] = model.encode(torch.from_numpy(arrs['x'])) # p_E(z) = \\int_x p(x) p_E(z|x)  \"z_pred\"\n",
    "arrs['x_reconstructed'] = model.decode(arrs['x_encoded'])     # p_D(y) = \\int_x \\int_z p(x) p_E(z|x) p_D(y|z) \"x_pred\"\n",
    "       \n",
    "# Feed the same z input to the decoder multiple times and study the stochastic output\n",
    "num_repeats = 100\n",
    "num_diff_zs = 100\n",
    "\n",
    "arrs['z_rep'] = np.array([np.repeat(arrs['z'][i:i+1], num_repeats, axis=0) for i in range(num_diff_zs)])       # \"z_fixed\"\n",
    "z_rep_tensor = torch.from_numpy(arrs['z_rep'])                                                                 # tmp\n",
    "arrs['z_decoded_rep'] = np.array([model.decode(z_rep_tensor[i]).detach().numpy() for i in range(num_diff_zs)]) # \"x_pred_truth_fixed\"\n",
    "arrs['x_rep'] = np.array([np.repeat(arrs['x'][i:i+1], num_repeats, axis=0) for i in range(num_diff_zs)])       # \"x_fixed\"\n",
    "\n",
    "# Convert all results to numpy arrays\n",
    "for (field, arr) in arrs.items():\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        arrs[field] = arr.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get mask corresponding to events which pass threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ttbar constraint function\n",
    "from ppttbar_constraints import threshold_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passing rate of z_decoded 0.68165\n",
      "passing rate of x_reconstructed 0.84145\n",
      "dict_keys(['x', 'z', 'z_decoded', 'x_encoded', 'x_reconstructed', 'z_rep', 'z_decoded_rep', 'x_rep', 'z_decoded_good_mask', 'x_reconstructed_good_mask'])\n"
     ]
    }
   ],
   "source": [
    "#-- Create new arrays from model output that passes cuts --#\n",
    "# Only save mask to dictionary\n",
    "\n",
    "arrs = all_arrs['val']\n",
    "\n",
    "for field in ('z_decoded', 'x_reconstructed'):\n",
    "    arr_raw = arrs[field] # Raw = unstandardized\n",
    "    \n",
    "    # Get mask for events that pass threshold constraint\n",
    "    good_mask = threshold_check(arr_raw)\n",
    "    print('passing rate of', field, good_mask.mean())\n",
    "\n",
    "    # Store masks that determine event-by-event passing\n",
    "    arrs[field+'_good_mask'] = good_mask\n",
    "\n",
    "# Print all keys in 'val' category\n",
    "print(arrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results saved at ./npzFiles/swae-lamb=0.001.npz\n"
     ]
    }
   ],
   "source": [
    "save_path = save_dir + save_filename\n",
    "np.savez(save_path, **all_arrs['val'])\n",
    "print('Model results saved at', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
