{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTUS | $p p > Z > e^+ e^-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook obtains the npz file of the trained model's results on validation data for the ablation study. Below are details about the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks applies OTUS to our first test case: $Z$ boson decaying into an electron ($e^-$) positron ($e^+$).\n",
    "\n",
    "Our physical latent-space is the $e^+$, $e^-$ 4-momentum information produced by the program MadGraph and our data-space data is the $e^+$, $e^-$ 4-momentum information produced by the program Delphes.\n",
    "\n",
    "We arrange this information into 8 dimensional vectors\n",
    "- Latent space (z): [$p^{\\mu}_{e^-}$,$p^{\\mu}_{e^+}$]\n",
    "- Data space (x):   [$p^{\\mu}_{e^-}$,$p^{\\mu}_{e^+}$]\n",
    "\n",
    "where $p^{\\mu}=[p_x, p_y, p_z, E]$ is the 4-momentum of the given particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Additional Losses and Constraints:\n",
    "We impose the following additional losses and constraints in this problem.\n",
    "\n",
    "First, we impose a constraint on the learned mappings via \"anchor losses\". This constrains the direction of the electron's 3-momenta when transforming from x-space to z-space and vice versa.\n",
    "\n",
    "Second, we explicitly enforce the Minkowski metric in the output of the networks. Namely, the networks predict the 3-momenta ($\\vec{p}$) of the particles. Energy information is then restored using the Minkowski metric: $E^2 = |\\vec{p}|^2 + m^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the paper for more details: https://arxiv.org/abs/2101.08944."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "root_dir = '../../../../'\n",
    "\n",
    "#-- Add utilityFunctions/ to easily use utility .py files --#\n",
    "import sys\n",
    "sys.path.append(os.path.join(root_dir, \"utilityFunctions/\"))\n",
    "\n",
    "#-- Determine if using GPU or CPU --#\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # Set to '-1' to disable GPU\n",
    "from configs import device, data_dims\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta =  0\n"
     ]
    }
   ],
   "source": [
    "#-- Set appropriate beta value --#\n",
    "# allBetas    = [0, 10, 20, 50, 100, 200]\n",
    "selectBetas = [0, 50, 100]\n",
    "beta = selectBetas[0]\n",
    "print('beta = ', beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data type:  float32\n"
     ]
    }
   ],
   "source": [
    "#-- Set directory short-cuts --#\n",
    "data_directory    = os.path.join(root_dir, \"data/\")\n",
    "dataset_name      = 'ppzee'\n",
    "\n",
    "#-- Set random seeds --#\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#-- Set data type --#\n",
    "from configs import float_type\n",
    "print('Using data type: ', float_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Validation Data for Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data total shapes:  (331699, 8) (331699, 8)\n",
      "z_train shape, x_train shape:  (291699, 8) (291699, 8)\n",
      "z_val   shape, x_val   shape:  (40000, 8) (40000, 8)\n"
     ]
    }
   ],
   "source": [
    "from func_utils import get_dataset, standardize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#-- Get training and validation dataset --#\n",
    "dataset = get_dataset(dataset_name, data_dir=data_directory) \n",
    "z_data, x_data = dataset['z_data'], dataset['x_data']\n",
    "\n",
    "MET = False # Exclude Missing Transverse Energy (MET) from x-space data\n",
    "if MET == False:\n",
    "    x_data = x_data[:, :-4]\n",
    "print(\"Data total shapes: \",z_data.shape, x_data.shape)\n",
    "\n",
    "x_dim = int(x_data.shape[1])\n",
    "z_dim = int(z_data.shape[1])\n",
    "\n",
    "#-- Split into training and validation sets --#\n",
    "train_size = 291699\n",
    "val_size = 40000  # Validation set used to evaluate/tune models\n",
    "\n",
    "x_train = x_data[:train_size, :]\n",
    "x_val = x_data[train_size:train_size+val_size, :]\n",
    "\n",
    "z_train = z_data[:train_size, :]\n",
    "z_val = z_data[train_size:train_size+val_size, :]\n",
    "\n",
    "#-- Convert data to proper type --#\n",
    "x_train, x_val, z_train, z_val = list(map(lambda x: x.astype(float_type), [x_train, x_val, z_train, z_val]))\n",
    "\n",
    "#-- Obtain mean and std information --#\n",
    "# This is needed to standardize/unstandardize data\n",
    "x_train_mean, x_train_std = np.mean(x_train, axis=0), np.std(x_train, axis=0) \n",
    "z_train_mean, z_train_std = np.mean(z_train, axis=0), np.std(z_train, axis=0)\n",
    "\n",
    "#-- Set evaluation parameters --#\n",
    "eval_batch_size = 20000  # Always use high batch size on validation set to accurately assess performance\n",
    "eval_loaders = DataLoader(dataset=x_val, batch_size=eval_batch_size, shuffle=True), \\\n",
    "               DataLoader(dataset=z_val, batch_size=eval_batch_size, shuffle=True)\n",
    "\n",
    "print(\"z_train shape, x_train shape: \", z_train.shape, x_train.shape)\n",
    "print(\"z_val   shape, x_val   shape: \", z_val.shape, x_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Define dictionary object for easy reference --#\n",
    "all_arrs = {'train': {}, 'val': {}}  # This will store all numpy arrays of interest\n",
    "all_arrs['train']['x'] = x_train\n",
    "all_arrs['train']['z'] = z_train\n",
    "all_arrs['val']['x']   = x_val\n",
    "all_arrs['val']['z']   = z_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Define target invariant masses --#\n",
    "x_inv_masses = np.zeros(2)\n",
    "z_inv_masses = np.zeros(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Specific Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from ppzee_utils import train_and_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Meta Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_noise = True  # Whether to use conditional Gaussian (instead of standard normal) for noise in enc/dec\n",
    "if cond_noise:\n",
    "    from models import CondNoiseAutoencoder\n",
    "    Autoencoder = CondNoiseAutoencoder  # Define alias \n",
    "else:\n",
    "    from models import Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Latent loss function: \n",
    "Finite sample approximation of Sliced Wasserstein Distance (SWD) between $p(z)$ and $p_E(z) = \\int_x p(x) p_E(z|x)$\n",
    "- $L_{latent}(Z, \\tilde{Z}) = \\frac{1}{L * M} \\sum_{l=1}^{L} \\sum_{m=1}^{M} c((\\theta_l \\cdot z_m)_{sorted}, (\\theta_l \\cdot \\tilde{z}_m)_{sorted})$ \n",
    "\n",
    "where $c(\\cdot, \\cdot) = |\\cdot - \\cdot|^2$\n",
    "\n",
    "###### Data loss function: \n",
    "- $L_{data}(X, \\tilde{X}) = \\frac{1}{M} \\sum_{m=1}^M c(x_m,  \\tilde{x}_m)$\n",
    "\n",
    "where $c(\\cdot, \\cdot) = |\\cdot - \\cdot|^2$\n",
    "\n",
    "###### Additional loss functions: \n",
    "Encoder and decoder anchor losses\n",
    "- $L_{A}(X, \\tilde{Z}) = \\frac{1}{M} \\sum_{m=1}^M c_A(x_m, \\tilde{z}_m)$\n",
    "- $L_{A}(Z, \\tilde{X}') = \\frac{1}{M} \\sum_{m=1}^M c_A(z_m, \\tilde{x}'_m)$\n",
    "\n",
    "see the paper for additional details about $c_A$.\n",
    "\n",
    "###### Full loss function:\n",
    "- $L_{tot} = \\beta L_{data}(X, \\tilde{X}) + \\lambda L_{latent}(Z, \\tilde{Z}) + \\nu_e L_{A}(X, \\tilde{Z}) + \\nu_d L_{A}(Z, \\tilde{X}')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Core Hyperparameters\n",
    "The hyperparameter definitions are as follows:\n",
    "- num_hidden_layers:    The number of hidden layers in both the encoder and decoder networks\n",
    "- dim_per_hidden_layer: The dimensions per hidden layer in both the encoder and decoder networks\n",
    "- lr: The learning rate of the networks\n",
    "- lamb: The $\\lambda$ coefficient in front of the latent loss term\n",
    "- num_slices: Number of random projections used for computing SWD\n",
    "- epochs: The number of epochs used during training\n",
    "\n",
    "Hyperparameters for other losses that were tried, but use during main training is currently discouraged:\n",
    "- tau: Coefficient in front of the alternate data-space loss (\"alt_x_loss\"), which is the SWD between $p(x)$ and $p_D(x):=\\int_z p(z) p_D(x|z)$\n",
    "- rho: Coefficient in front of an additional decoder constraint loss (based on soft-penalty approach to learning hard thresholds/ttbar_constraints)\n",
    "\n",
    "###### Joint Training Hyperparameters\n",
    "\n",
    "- beta: Coefficient in front of data loss, $L_{data}$     \n",
    "- nu_e: Coefficient in front of the encoder \"anchor loss\" \n",
    "- nu_d: Coefficient in front of the decoder \"anchor loss\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_hidden_layers': 1,\n",
    "    'dim_per_hidden_layer': 128,\n",
    "    'lr': 0.001,\n",
    "    'beta': beta,  # weight of the data reconstruction loss\n",
    "    'lamb': 1.,    # weight of the latent space matching loss\n",
    "    'tau': 0,  \n",
    "    'rho': 0,  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_hidden_layers, dim_per_hidden_layer = config['num_hidden_layers'], config['dim_per_hidden_layer']\n",
    "hidden_layer_dims = num_hidden_layers * [dim_per_hidden_layer]\n",
    "\n",
    "activation = torch.nn.ReLU\n",
    "sigma_fun = 'softplus'  # Default is 'exp'\n",
    "model = Autoencoder(x_dim=x_dim, z_dim=z_dim, hidden_layer_dims=hidden_layer_dims, raw_io=True,\n",
    "                    x_stats=np.stack([x_train_mean, x_train_std]), z_stats=np.stack([z_train_mean, z_train_std]),\n",
    "                    x_inv_masses=x_inv_masses, z_inv_masses=z_inv_masses,\n",
    "                    stoch_enc=True, stoch_dec=True, activation=activation, sigma_fun=sigma_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CondNoiseAutoencoder(\n",
       "  (encoder): CondNoiseMLP(\n",
       "    (sigma_fun): Softplus(beta=1, threshold=20)\n",
       "    (output_nn): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=6, bias=True)\n",
       "    )\n",
       "    (cond_noise_nn): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): CondNoiseMLP(\n",
       "    (sigma_fun): Softplus(beta=1, threshold=20)\n",
       "    (output_nn): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=6, bias=True)\n",
       "    )\n",
       "    (cond_noise_nn): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Trained Model on Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier downstream analysis we also evaluate the trained model on our testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CondNoiseAutoencoder(\n",
       "  (encoder): CondNoiseMLP(\n",
       "    (sigma_fun): Softplus(beta=1, threshold=20)\n",
       "    (output_nn): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=6, bias=True)\n",
       "    )\n",
       "    (cond_noise_nn): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): CondNoiseMLP(\n",
       "    (sigma_fun): Softplus(beta=1, threshold=20)\n",
       "    (output_nn): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=6, bias=True)\n",
       "    )\n",
       "    (cond_noise_nn): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Reset random seeds --#\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#-- Evaluate trained model on validation dataset --#\n",
    "\n",
    "# Use CPU instead of GPU\n",
    "model.to('cpu')\n",
    "model.encoder.output_stats.to('cpu')\n",
    "model.decoder.output_stats.to('cpu')\n",
    "\n",
    "#-- Set save directory location for npz files --#\n",
    "save_dir      = './npzFiles/'\n",
    "save_filename = f'swae-beta={beta}.npz'\n",
    "\n",
    "#-- Load model's trained weights and set to evaluation mode --#\n",
    "model.load_state_dict(torch.load(f'swae-beta={beta}.pkl', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 8) (40000, 8)\n"
     ]
    }
   ],
   "source": [
    "#-- Get validation dataset set into dictionary --#\n",
    "# z_val and x_val already defined from above\n",
    "print(all_arrs['val']['x'].shape, all_arrs['val']['z'].shape)\n",
    "\n",
    "# Evaluate trained model on validation dataset\n",
    "arrs = all_arrs['val']\n",
    "\n",
    "arrs['z_decoded'] = model.decode(torch.from_numpy(arrs['z'])) # p_D(x) = \\int_z p(z) p_D(x|z)  \"x_pred_truth\"\n",
    "arrs['x_encoded'] = model.encode(torch.from_numpy(arrs['x'])) # p_E(z) = \\int_x p(x) p_E(z|x)  \"z_pred\"\n",
    "arrs['x_reconstructed'] = model.decode(arrs['x_encoded'])     # p_D(y) = \\int_x \\int_z p(x) p_E(z|x) p_D(y|z) \"x_pred\"\n",
    "       \n",
    "# Feed the same z input to the decoder multiple times and study the stochastic output\n",
    "num_repeats = 100\n",
    "num_diff_zs = 100\n",
    "\n",
    "arrs['z_rep'] = np.array([np.repeat(arrs['z'][i:i+1], num_repeats, axis=0) for i in range(num_diff_zs)])       # \"z_fixed\"\n",
    "z_rep_tensor = torch.from_numpy(arrs['z_rep'])                                                                 # tmp\n",
    "arrs['z_decoded_rep'] = np.array([model.decode(z_rep_tensor[i]).detach().numpy() for i in range(num_diff_zs)]) # \"x_pred_truth_fixed\"\n",
    "arrs['x_rep'] = np.array([np.repeat(arrs['x'][i:i+1], num_repeats, axis=0) for i in range(num_diff_zs)])       # \"x_fixed\"\n",
    "\n",
    "# Convert all results to numpy arrays\n",
    "for (field, arr) in arrs.items():\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        arrs[field] = arr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results saved at ./npzFiles/swae-beta=0.npz\n"
     ]
    }
   ],
   "source": [
    "save_path = save_dir + save_filename\n",
    "np.savez(save_path, **all_arrs['val'])\n",
    "print('Model results saved at', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
